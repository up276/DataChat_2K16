{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import inflect\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load any words embedding\n",
    "# Here we use word2vec Google pretrained model\n",
    "# The link to download those embeddings can be found on this page :\n",
    "# https://github.com/mmihaltz/word2vec-GoogleNews-vectors  (download .gz file in the README)\n",
    "\n",
    "path = './model/GoogleNews-vectors-negative300.bin'\n",
    "model = gensim.models.Word2Vec.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data \n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning strings\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip('\\n').strip().lower()\n",
    "\n",
    "def load_test_data(path, file_name):\n",
    "    df = pd.read_csv(path + file_name, sep='\\t', header=None)\n",
    "    df.columns = ['hardcoded_question', 'user_question']\n",
    "    for i in df.index:\n",
    "        df['hardcoded_question'][i] = clean_str(df['hardcoded_question'][i])\n",
    "        df['user_question'][i] = clean_str(df['user_question'][i])\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the test file which is a .tsv file with format : hardcoded_question \\t user_question \n",
    "path = 'data2/'\n",
    "file_name = 'test_set2.tsv'\n",
    "data = load_test_data(path, file_name)\n",
    "questions = data['hardcoded_question'].tolist() + data['user_question'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_pos = ['NN', 'NNS', 'VB', 'JJ']\n",
    "\n",
    "def filter_pos(question):\n",
    "    '''\n",
    "    From a sentence, return list of tuples [(word, POS)] if POS in self.keep_pos (by default : noun(s))\n",
    "    '''\n",
    "    question_tokens = nltk.word_tokenize(question)\n",
    "    question_tagged = nltk.pos_tag(question_tokens)\n",
    "    question_tagged_pos = [question_tagged[i] \n",
    "                            for i in range(len(question_tagged)) \n",
    "                            if question_tagged[i][1] \n",
    "                            in keep_pos]\n",
    "    return question_tagged_pos\n",
    "\n",
    "def plural_to_singular(tagged_pos):\n",
    "    '''\n",
    "    Putting all nouns to singular form (because similarity(A,A') > similarity(A,As'))\n",
    "    '''\n",
    "    p = inflect.engine()\n",
    "    q_singular = []\n",
    "    for pos in tagged_pos:\n",
    "        if pos[1] != 'NNS':\n",
    "            q_singular.append(pos[0]) \n",
    "        else:\n",
    "            q_singular.append(p.singular_noun(pos[0]))\n",
    "    return q_singular\n",
    "\n",
    "def embed_question(question):\n",
    "    question_tagged_nouns = filter_pos(question)\n",
    "    question_nouns = plural_to_singular(question_tagged_nouns)\n",
    "    embedding = np.zeros(model['random'].shape)\n",
    "    i=0.0\n",
    "    for word in question_nouns:\n",
    "        if word in model.vocab:\n",
    "            i+=1.0\n",
    "            embedding += model[word]\n",
    "    embedding /= i\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded_questions = [embed_question(q) for q in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# saving output files\n",
    "with open ('data2/question_embeddings.tsv', 'w') as embeddings:\n",
    "    writer = csv.writer(embeddings, delimiter='\\t')\n",
    "    for eq in embedded_questions:\n",
    "        writer.writerow(eq)\n",
    "        \n",
    "with open ('data2/questions.tsv', 'w') as embeddings:\n",
    "    for q in questions:\n",
    "        embeddings.write(q+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
