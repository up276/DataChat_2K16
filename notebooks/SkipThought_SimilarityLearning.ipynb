{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-thought vectors & Similarity Learning\n",
    "\n",
    "The goal of this notebook is to experiment with the opportunities that sentence2vec (here skip-thought) embedding offer for the task of question similarities . \n",
    "\n",
    "NB : You need to download the skip-thought code + the pre-trained models to run this ipynb (cf github rep link below)\n",
    "\n",
    "Usefull links:\n",
    "- Paper : https://arxiv.org/abs/1506.06726\n",
    "- Github rep : https://github.com/ryankiros/skip-thoughts\n",
    "        \n",
    "First, we use a naive approach in which we just use as is the sentence embeddings provided by the skip-thouth \n",
    "model, i.e. provided 2 questions, we compute their similarity as the cosine similarity between their vectorize representation.\n",
    "It turns out it does not work that well : 2 questions asked with a similar grammatical structure but different meaning will be attributed a relatively high score, so it does not allow to distinguish that from a semantically similar question asked in a grammatically different way. The reason for that seems to be that sentence2vec-like models do not only encode semantic but also sentences structure.\n",
    "\n",
    "To solve this issue, we try to learn a projection matrix in hope to learn which dimensions of the encoding would be \n",
    "more related to semantic and which one related to structure (this is part of similarity learning). \n",
    "I.e. instead of computing <question1, question2> to get a similarity score, we compute user_question.W.hc_question \n",
    "and try to supervisely learn W (with regularization, and initializing M as the identity matrix).\n",
    "\n",
    "For more on similarity learning, check wikipedia page :\n",
    "https://en.wikipedia.org/wiki/Similarity_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append('/Users/vincentchabot/Desktop/capstone/skip_thoughts/skip-thoughts-master/')\n",
    "import skipthoughts\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model parameters...\n",
      "Compiling encoders...\n",
      "Loading tables...\n",
      "Packing up...\n"
     ]
    }
   ],
   "source": [
    "# make sure to download & provide the correct path to your /models repository in skipthoughts.py\n",
    "model = skipthoughts.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1 Similarity on sentences skip-thought embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Toy example\n",
    "X = ['what is the number of cars in the base ?',\n",
    "     'how many vehicles do i have in my database ?',\n",
    "     'what is the number of users in the base ?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create array of the embeded sentences\n",
    "vectors_X = skipthoughts.encode(model, X, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_cosine_sim between :\n",
      "\n",
      "what is the number of cars in the base ?\n",
      "and\n",
      "how many vehicles do i have in my database ?\n",
      "similarity:  0.819562\n",
      "\n",
      "sentence_cosine_sim between :\n",
      "\n",
      "what is the number of cars in the base ?\n",
      "and\n",
      "what is the number of users in the base ?\n",
      "similarity:  0.95662\n"
     ]
    }
   ],
   "source": [
    "def sentence_cosine_sim(vectors, sent1_index, sent2_index):\n",
    "    '''\n",
    "    find pairwise cosine similarity between sentence1 and sentence2\n",
    "    '''\n",
    "    s1, s2 = vectors[sent1_index], vectors[sent2_index]\n",
    "    cosine_similarity = np.dot(s1, s2)/(np.linalg.norm(s1) * np.linalg.norm(s2))\n",
    "    return cosine_similarity\n",
    "\n",
    "print 'sentence_cosine_sim between :\\n'\n",
    "print X[0] \n",
    "print 'and'\n",
    "print X[1]\n",
    "print \"similarity: \", sentence_cosine_sim(vectors_X, 0, 1)\n",
    "\n",
    "print '\\nsentence_cosine_sim between :\\n'\n",
    "print X[0] \n",
    "print 'and'\n",
    "print X[2]\n",
    "print \"similarity: \", sentence_cosine_sim(vectors_X, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --> 2 questions asked with a similar grammatical structure but different meaning will be attributed a relatively high score, so it does not allow to distinguish that from a semantically similar question asked in a grammatically different way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2 Similarity learning on top of skip-thought embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Load and clean the data, create train/test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "\n",
      "loading hardcoded_question_set...\n",
      "\n",
      "loading test set...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning strings\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip('\\n').strip().lower()\n",
    "\n",
    "def load_data(path, file_name):\n",
    "    df = pd.read_csv(path + file_name, sep='\\t', header=None)\n",
    "    df.columns = ['hardcoded_question']\n",
    "    for i in df.index:\n",
    "        df['hardcoded_question'][i] = clean_str(df['hardcoded_question'][i])\n",
    "    return df\n",
    "\n",
    "def load_test_data(path, file_name):\n",
    "    df = pd.read_csv(path + file_name, sep='\\t', header=None)\n",
    "    df.columns = ['hardcoded_question', 'user_question']\n",
    "    for i in df.index:\n",
    "        df['hardcoded_question'][i] = clean_str(df['hardcoded_question'][i])\n",
    "        df['user_question'][i] = clean_str(df['user_question'][i])\n",
    "    return df \n",
    "\n",
    "# upload data\n",
    "print \"loading data...\\n\"\n",
    "print \"loading hardcoded_question_set...\\n\"\n",
    "path_train = '/Users/vincentchabot/Desktop/capstone/Data2/'\n",
    "file_name_train = 'test2.txt' \n",
    "hardcoded_question_set = load_data(path_train, file_name_train)\n",
    "print \"loading test set...\\n\"\n",
    "path_test = '/Users/vincentchabot/Desktop/capstone/Data2/'\n",
    "file_name_test = 'test_set2.tsv' \n",
    "test_set = load_test_data(path_test, file_name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hardcoded_question</th>\n",
       "      <th>user_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do i have risky customers \\?</td>\n",
       "      <td>what are the clients at risk \\?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how can i change my password \\?</td>\n",
       "      <td>i lost my password how can i have a new one \\?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>are there blocked invoices \\?</td>\n",
       "      <td>how many invoices are blocked \\?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how many vehicles do i have in my database \\?</td>\n",
       "      <td>what is the number of cars in the base \\?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>does snow fall in egypt \\?</td>\n",
       "      <td>is there snow in egypt \\?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              hardcoded_question  \\\n",
       "0                   do i have risky customers \\?   \n",
       "1                how can i change my password \\?   \n",
       "2                  are there blocked invoices \\?   \n",
       "3  how many vehicles do i have in my database \\?   \n",
       "4                     does snow fall in egypt \\?   \n",
       "\n",
       "                                    user_question  \n",
       "0                 what are the clients at risk \\?  \n",
       "1  i lost my password how can i have a new one \\?  \n",
       "2                how many invoices are blocked \\?  \n",
       "3       what is the number of cars in the base \\?  \n",
       "4                       is there snow in egypt \\?  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = test_set[0:19]\n",
    "test = test_set[20:27]\n",
    "\n",
    "# reindex test dataframe\n",
    "test.index = range(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here, we create a hardcoded_question / user_question pair set\n",
    "# we create all the possible question pairs in order to have as many pairs as possible to train on.\n",
    "# For each hardcoded question, only 1 pair will be correct and all the other will be false.\n",
    "def create_sentence_pairset(data):\n",
    "    '''\n",
    "    Output =  {(hardcoded_question, user_question) : ((hardcoded_question_vector, user_question_vector, correct))}\n",
    "    with correct being 1 if the user question corresponds to the hardcoded question, 0 otherwise\n",
    "    '''\n",
    "    sentence_pairset = {}\n",
    "    hqs = data['hardcoded_question'].tolist()\n",
    "    uqs = data['user_question'].tolist()\n",
    "    hqs_vectors = skipthoughts.encode(model, hqs, verbose=False)\n",
    "    uqs_vectors = skipthoughts.encode(model, uqs, verbose=False)\n",
    "    # Normalize\n",
    "    for i in range(hqs_vectors.shape[0]):\n",
    "        hqs_vectors[i] /= np.linalg.norm(hqs_vectors[i])\n",
    "        uqs_vectors[i] /= np.linalg.norm(hqs_vectors[i])\n",
    "    for hq in hqs:\n",
    "        for uq in uqs:\n",
    "            if hqs.index(hq) == uqs.index(uq):\n",
    "                sentence_pairset[(hq,uq)] = (hqs_vectors[hqs.index(hq)], uqs_vectors[uqs.index(uq)], 1.0)\n",
    "            else:\n",
    "                sentence_pairset[(hq,uq)] = (hqs_vectors[hqs.index(hq)], uqs_vectors[uqs.index(uq)], 0.0)\n",
    "    return sentence_pairset\n",
    "\n",
    "train_sentence_pairset = create_sentence_pairset(train)\n",
    "test_sentence_pairset = create_sentence_pairset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Learning the projection matrix W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def compute_square_loss(sentence_pairset, Lambda, W, embedding_length):\n",
    "    loss = 0\n",
    "    values_computed = 0.0\n",
    "    for value in sentence_pairset.itervalues():\n",
    "        \n",
    "        # Since there are way more false pairs than true pairs, we can balance this ratio by taking only \n",
    "        # 1 false pair every x false pairs. Otherwise, the final W matrix will be biaised toward false pairs\n",
    "        # and will only produce very small scores\n",
    "        compute_gradient = True\n",
    "        '''\n",
    "        correct = value[2]\n",
    "        if correct == 1.0:\n",
    "            compute_gradient = True\n",
    "        elif correct == 0.0:\n",
    "            if randint(0,9)==0:\n",
    "                compute_gradient = True\n",
    "            else:\n",
    "                compute_gradient = False\n",
    "        '''\n",
    "        \n",
    "        if compute_gradient: \n",
    "            hq_vector = value[0].reshape((embedding_length,1))\n",
    "            uq_vector = value[1].reshape((embedding_length,1))\n",
    "            correct = value[2]\n",
    "            hq_vectorT = np.transpose(hq_vector)\n",
    "            uq_vectorT = np.transpose(uq_vector)\n",
    "            loss += np.linalg.norm(correct - np.dot(hq_vectorT, np.dot(W,uq_vector)))\n",
    "            values_computed += 1.0\n",
    "            \n",
    "    loss /= 2*values_computed\n",
    "    loss += Lambda*(np.linalg.norm(W))**2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_regularized_square_loss_gradient(sentence_pairset, embedding_length, Lambda, W):\n",
    "    \"\"\"\n",
    "    Compute the gradient of L2-regularized square loss function \n",
    "    \"\"\"\n",
    "    gradient = np.zeros((embedding_length, embedding_length))\n",
    "    values_computed = 0.0\n",
    "    for value in sentence_pairset.itervalues():\n",
    "        \n",
    "        # Since there are way more false pairs than true pairs, we can balance this ratio by taking only \n",
    "        # 1 false pair every x false pairs. Otherwise, the final W matrix will be biaised toward false pairs\n",
    "        # and will only produce very small scores\n",
    "        compute_gradient = True\n",
    "        '''\n",
    "        correct = value[2]\n",
    "        if correct == 1.0:\n",
    "            compute_gradient = True\n",
    "        elif correct == 0.0:\n",
    "            if randint(0,9)==0:\n",
    "                compute_gradient = True\n",
    "            else:\n",
    "                compute_gradient = False\n",
    "        '''\n",
    "        \n",
    "        if compute_gradient: \n",
    "            hq_vector = value[0].reshape((embedding_length,1))\n",
    "            uq_vector = value[1].reshape((embedding_length,1))\n",
    "            correct = value[2]\n",
    "            hq_vectorT = np.transpose(hq_vector)\n",
    "            uq_vectorT = np.transpose(uq_vector)\n",
    "            gradient += - np.dot(hq_vector, uq_vectorT) * (correct - np.dot(hq_vectorT, np.dot(W,uq_vector)))\n",
    "            values_computed += 1.0\n",
    "        \n",
    "    gradient /= values_computed\n",
    "    gradient += 2*Lambda*W\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def regularized_grad_descent(sentence_pairset, alpha=0.1, Lambda=10, num_iter=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sentence_pairset = sentence pairs + correct value (1 if they sementically correspond, 0 otherwise)\n",
    "        alpha - step size in gradient descent\n",
    "        Lambda - the regularization coefficient\n",
    "        num_iter - number of iterations to run \n",
    "        \n",
    "    Returns:\n",
    "        W_hist - the history of parameter vector, 2D numpy array of size (num_iter+1, num_features) \n",
    "        loss_hist - the history of regularized loss value, 1D numpy array\n",
    "    \"\"\"\n",
    "    # initialize W as Identity matrix\n",
    "    embedding_length = skipthoughts.encode(model, ['random'], verbose=False).shape[1]\n",
    "    W = np.identity(embedding_length)\n",
    "    \n",
    "    # initialize W_hist and loss_hist\n",
    "    W_hist = np.zeros((num_iter+1, embedding_length, embedding_length)) \n",
    "    loss_hist = np.zeros(num_iter+1)  \n",
    "    W_hist[0]=W\n",
    "    loss_hist[0]=compute_square_loss(sentence_pairset, Lambda, W, embedding_length)\n",
    "    \n",
    "    for i in range(1,num_iter+1):\n",
    "        print 'iteration number ', i\n",
    "        W = W - alpha*compute_regularized_square_loss_gradient(sentence_pairset, embedding_length, Lambda, W)\n",
    "        # Normalize\n",
    "        W_norm = np.linalg.norm(W)/np.linalg.norm(W_hist[0])\n",
    "        W = W/W_norm\n",
    "        W_hist[i]=W\n",
    "        loss_hist[i]=compute_square_loss(sentence_pairset, Lambda, W, embedding_length)\n",
    "    return W_hist, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number  1\n",
      "iteration number  2\n",
      "iteration number  3\n",
      "iteration number  4\n",
      "iteration number  5\n",
      "iteration number  6\n",
      "iteration number  7\n",
      "iteration number  8\n",
      "iteration number  9\n",
      "iteration number  10\n",
      "iteration number  11\n",
      "iteration number  12\n",
      "iteration number  13\n",
      "iteration number  14\n",
      "iteration number  15\n"
     ]
    }
   ],
   "source": [
    "# Learn W\n",
    "# Multiplying num_iter by 15 because we are only taking 1 false pair every 15 false pairs in training \n",
    "n_iter = 15\n",
    "alph = 0.1\n",
    "W_hist, loss_hist = regularized_grad_descent(train_sentence_pairset, alpha=alph, Lambda=1, num_iter=n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --> Let's check how the learning went"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_test_loss(sentence_pairset, W_hist, Lambda, num_iter):\n",
    "    #Initialize loss_hist\n",
    "    test_loss_hist = np.zeros(num_iter+1)   \n",
    "    for i in range(num_iter+1):\n",
    "        test_loss_hist[i]=compute_square_loss(sentence_pairset, Lambda, W_hist[i], embedding_length)\n",
    "    return test_loss_hist\n",
    "\n",
    "n_iter = 15\n",
    "embedding_length = skipthoughts.encode(model, ['random'], verbose=False).shape[1]\n",
    "test_loss_hist = compute_test_loss(test_sentence_pairset, W_hist, Lambda=1, num_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEVCAYAAADkckIIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRtb+ZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1110e6690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_hist, label='train loss')\n",
    "plt.plot(test_loss_hist, label='validation loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('number of iterations step')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the loss plot, the learning seems to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) Let's see the results on the toy example from the begining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = ['what is the number of cars in the base ?',\n",
    "     'how many vehicles do i have in my database ?',\n",
    "     'what is the number of users in the base ?']\n",
    "\n",
    "vectors_X = skipthoughts.encode(model, X, verbose=False)\n",
    "\n",
    "def sentence_cosine_sim_with_W(vectors, sent1_index, sent2_index, W):\n",
    "    '''\n",
    "    find pairwise cosine similarity between sentence1 and sentence2\n",
    "    '''\n",
    "    # first, embed sentences\n",
    "    s1, s2 = vectors[sent1_index], vectors[sent2_index]\n",
    "    # second, compute cosine similarity\n",
    "    cosine_similarity = np.dot(np.transpose(s1), np.dot(W,s2))/(np.linalg.norm(s1) * np.linalg.norm(s2) )\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_cosine_sim between :\n",
      "\n",
      "what is the number of cars in the base ?\n",
      "and\n",
      "how many vehicles do i have in my database ?\n",
      "similarity:  0.161234690071\n",
      "\n",
      "sentence_cosine_sim between :\n",
      "\n",
      "what is the number of cars in the base ?\n",
      "and\n",
      "what is the number of users in the base ?\n",
      "similarity:  0.263535810462\n"
     ]
    }
   ],
   "source": [
    "print 'sentence_cosine_sim between :\\n'\n",
    "print X[0] \n",
    "print 'and'\n",
    "print X[1]\n",
    "print \"similarity: \", sentence_cosine_sim_with_W(vectors_X, 0, 1, W_hist[-1])\n",
    "\n",
    "print '\\nsentence_cosine_sim between :\\n'\n",
    "print X[0] \n",
    "print 'and'\n",
    "print X[2]\n",
    "print \"similarity: \", sentence_cosine_sim_with_W(vectors_X, 0, 2, W_hist[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### --> We observe that scores are both lower but the score for a true pair is still not greater than the score for a false pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "1) The fact that from the very little available data we had we constructed a data set that has 26 times more false pairs than true pairs, the framework learnt a W that tends to lower all the scores and tends to predict false all the time. Using the AUC as metric should give better results.\n",
    "\n",
    "2) Learning a 3800x3800 matrix from 20 examples is of course way too ambitious. But at least the similarity learning framework we've built shows that it is possible to learn such a projection matrix using sentence embedding like skip-thought and might be reuse once more data is available. Furthermore, such a framework could also be used on other sentence embeddings we built otherwise, like the one using word2vec embeddings + CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
